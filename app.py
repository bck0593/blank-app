import os
import re
import requests
import time
import zipfile
import io
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from datetime import datetime
import streamlit as st

# ã‚¢ãƒ—ãƒªåè¨­å®š
st.set_page_config(page_title="PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ€ãƒ¼", page_icon="ğŸ“¥")

# å…¥åŠ›ã•ã‚ŒãŸURL
BASE_URL = st.text_input("PDFã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸã„URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", "https://www.ncbank.co.jp/news/release/2024/")

# ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€
SAVE_FOLDER = "./PDF"
os.makedirs(SAVE_FOLDER, exist_ok=True)

# PDFãƒªãƒ³ã‚¯ã®å–å¾—
def get_pdf_links(base_url):
    response = requests.get(base_url)
    if response.status_code != 200:
        st.error(f"ã‚¨ãƒ©ãƒ¼: {BASE_URL}ãŒæ­£ã—ãèª­ã¿è¾¼ã¾ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
        return []

    soup = BeautifulSoup(response.content, "html.parser")
    pdf_data = []
    
    for link in soup.find_all("a", href=True):
        href = link["href"]
        if href.endswith(".pdf"):
            full_url = urljoin(BASE_URL, href)
            title = link.get_text(strip=True)
            if not title:
                title = "unknown_title"

            # ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡ºï¼ˆä»®ã«yyyy-mm-ddå½¢å¼ã‚’æƒ³å®šï¼‰
            date_str = re.search(r'(\d{4}-\d{2}-\d{2})', title)
            date = datetime.strptime(date_str.group(1), '%Y-%m-%d') if date_str else None

            # ã‚¿ã‚¤ãƒˆãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ«åã¨ã—ã¦ä½¿ãˆã‚‹ã‚ˆã†ã«æ•´å½¢
            safe_title = re.sub(r'[\\/*?:"<>|]', "", title)
            safe_title = re.sub(r'[ï¼»ï¼½]', '', safe_title)  # å…¨è§’ã®è§’ã‹ã£ã“ã‚’å‰Šé™¤
            safe_title = safe_title.strip()

            if len(safe_title) > 100:
                safe_title = safe_title[:100]

            pdf_data.append((full_url, safe_title, date))

    return pdf_data

# PDFãƒªãƒ³ã‚¯ã‚’å–å¾—
pdf_data = get_pdf_links(BASE_URL)

# PDFãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã€é¸æŠè‚¢ã‚’æä¾›
if pdf_data:
    # PDFã®é¸æŠ
    pdf_titles = [pdf[1] for pdf in pdf_data]
    selected_pdfs = st.multiselect("ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹PDFã‚’é¸æŠã—ã¦ãã ã•ã„", pdf_titles)

    # å®Ÿè¡Œãƒœã‚¿ãƒ³
    if st.button("å®Ÿè¡Œ"):
        if selected_pdfs:
            # ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
            zip_buffer = io.BytesIO()
            with zipfile.ZipFile(zip_buffer, "w", zipfile.ZIP_DEFLATED) as zip_file:
                for pdf_title in selected_pdfs:
                    # å¯¾å¿œã™ã‚‹PDFã®URLã‚’å–å¾—
                    pdf_url = next(pdf[0] for pdf in pdf_data if pdf[1] == pdf_title)
                    pdf_response = requests.get(pdf_url)
                    
                    if pdf_response.status_code == 200:
                        # PDFã‚’ZIPãƒ•ã‚¡ã‚¤ãƒ«ã«è¿½åŠ 
                        file_name = f"{pdf_title}.pdf"
                        zip_file.writestr(file_name, pdf_response.content)
                    else:
                        st.error(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {pdf_url}")

                    # ã‚µãƒ¼ãƒãƒ¼è² è·ã‚’é¿ã‘ã‚‹ãŸã‚ã®1ç§’ã‚¹ãƒªãƒ¼ãƒ—
                    time.sleep(1)

            # ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ã¨ã—ã¦è¡¨ç¤º
            zip_buffer.seek(0)
            st.download_button(
                label="PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=zip_buffer,
                file_name="pdf_files.zip",
                mime="application/zip"
            )
        else:
            st.warning("PDFã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
else:
    st.warning("PDFã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
